---
title: "Código Modelos"
author: "Irene Extremera Serrano"
date: "6/12/2020"
output: html_document
---

````{r, warning=FALSE,message=FALSE,results='hide'}
library(sp) #Tabajar con objetos de tipo espacial
library(rgdal) #Para que funcione sdmpredictors
library(carData)
library(car)
library(nlme)
library(gstat)
library(sf)
library(spData)
library(spdep)
library(lattice)
library(survival)
library(Formula)
library(ggplot2)
library(Hmisc)
library(raster) #Para poder trabajar con objetos tipo raster
library(leaflet)
library(GGally)
library(maptools)
library(corrplot)
library(rgeos)
library(maptools) #Cargar mapas
library(dismo) #Para poder trabajar con bioclim
library(sdmpredictors) #Para descargarme las variables ambientales
library(PresenceAbsence)
```

###########################################
##### --- Cargar la base de datos --- #####
###########################################

```{r}
# Descargo la base de datos de aquamaps
# Tengo en la base de datos por defecto en el cabecero una descripción que hay que quitar.
data <- read.csv('Datos/Anchoas_Aqua.csv',TRUE,",")

# Miro las variables que tiene la base de datos y me quedo con las que me interesan
names(data)
data <- data[, c(3,4)]

# Me cercioro de que efectivamente me he quedado con las que necesito
names(data)

# Les cambio el nombre y el orden
colnames(data)<- c('Lat','Lon')
data<- cbind(data$Lon, data$Lon)
colnames(data)<- c('Lon','Lat')

# Descargo el mapa del mundo
data(wrld_simpl) #Cargo el mapa del mundo
```

```{r}
# Pinto los datos para ver por dónde están las anchoas. Corto por Atlántico Norte y Mediterraneo.
plot(wrld_simpl, xlim= c(-25,40),ylim=c(30,65), axes=TRUE,col="light yellow")
points(data$Lon, data$Lat, col="orange", pch=20, cex=0.75)
points(data$Lon, data$Lat, col="red", cex=0.75)

# Identifico los puntos que no están en el Mediterraneo y en el norte de Europa que me podrían dar problemas para modelizar 
identify(data$Lon, data$Lat) 

#No parece que haya errores en los datos porque al pintarlos aparecen en las mismas zonas que en las que aparecen en el mapa de: https://www.gbif.org/species/2414031

# Los puntos de África voy a quitarlos porque van a ser difíciles de modelizar, por lo que me quedo solo con Europa.Lo mismo con los datos del Mar Muerto y mar Egeo.
data <- data[c(-342, -348, -351, -357, -364, -368, -372, -378, -394, -397, -398,-152, -153, -160, -163, -164, -165, -166, -234, -237, -242, -243, -248, -250, -253, -259, -262, -263, -268, -273, -276, -278, -282, -285, -286, -291, -292, -293, -296, -297, -301, -304, -307, -308, -309, -313, -315, -317, -319, -320, -329, -336, -338),]

# Vuelvo a hacer el plot para cerciorarme que se han eliminado correctamente.
plot(wrld_simpl, xlim= c(-25,40),ylim=c(30,65), axes=TRUE,col="light yellow")
points(data$Lon, data$Lat, col="orange", pch=20, cex=0.75)
points(data$Lon, data$Lat, col="red", cex=0.75)

# Me quedaban 3 puntos 
data <- data[c(-253, -265, -270),]

# Vuelvo a hacer el plot para cerciorarme que se han eliminado correctamente.
plot(wrld_simpl, xlim= c(-25,40),ylim=c(30,65), axes=TRUE,col="light yellow")
points(data$Lon, data$Lat, col="orange", pch=20, cex=0.75)
points(data$Lon, data$Lat, col="red", cex=0.75)
```

```{r}
#Miro si hay duplicados
dups2 <- duplicated(data[, c("Lon","Lat")]) 

#Veo cuántos duplicados 
sum(dups2) 

#Verifico si son diferentes antes de eliminarlos.
data <- data[!dups2, ] 

# Guardo los datos
data <- cbind(data$Lon,data$Lat)
write.csv(data,"Datos/Anchoas_Aqua.csv")
data <- read.csv('Datos/Anchoas_Aqua.csv',TRUE,",")
data<-data[,-1] #Me aparece una columna de índices
```

```{r}
### NO ME FUNCIONA, SE QUEDA TODO METIDO EN UN MISMO RASTER Y NO COMO RASTERS DIFERENTES

# Las variables explicativas que voy a usar son: slainidad, oxígeno disuelto, temperatura de la superficie, clorofila media (de la base de datos "Bio-ORACLE") y batimetría (de la base de datos MARSPEC).
# Me las descargo
# bathy<- load_layers(c("MS_bathy_5m"))
# ocean<- load_layers(c("BO2_chlomean_ss"))
# ocean2<- load_layers(c("BO2_ppmean_ss","BO2_dissoxmean_ss","BO2_salinitymean_ss","BO2_tempmean_ss"))


# Uso la función resample para igualar el número de filas 
# bathy=resample(bathy,ocean2)
# ocean=resample(ocean,ocean2)

# Meto los predictores en una misma base de datos

# predictors=stack(bathy,ocean,ocean2)

# Doy nombres a las variables explicativas
# names(predictors) <- c("bathy","chlomean","ppmean","ox","sss","sst")

# Me quedo con los valores de los predictores de la zona en la que voy a trabajar
# ext<-extent(-25,40,30,65)
# predictors<-crop(predictors,ext)

# Me cercioro de que los predictores cogen el area que quiero
# plot(predictors)

# Me guardo las variables en un raster para no tener que estar descargándolas
# writeRaster(predictors, filename="predictors.tif", format="GTiff", overwrite=TRUE)

# Llamo a mis predictores
# predictors<- raster("predictors.tif")

# Estandarizo las variables para que estén en la misma escala
# predictors2 <- scale(predictors)
# round(apply(values(predictors2), 2, summary), 4) 

# Compruebo que puedo verlos
# par(mfrow=c(2,2))
# plot(predictors2)
```

```{r}
# A la hora de seleccionar variables nos hemos guiado por la biología de la especie: 
# Llamo a los rasters por separado
bathy<- load_layers(c("MS_bathy_5m"))
chlomean<- load_layers(c("BO2_chlomean_ss"))
ppmean<- load_layers(c("BO2_ppmean_ss"))
tempmean <- load_layers(c("BO2_tempmean_ss"))
odismean<- load_layers(c("BO2_dissoxmean_ss"))
salinity <- load_layers(c("BO2_salinitymean_ss"))

# La longitud de bathy y chlomean es diferente por lo que la igualo a la del resto con la función resample
bathy=resample(bathy,salinity)
chlomean=resample(chlomean,salinity)

# Corto los rasters para luego guardarlos y que así me ocupen menos 
ext<-extent(-25,40,30,65)
bathy<-crop(bathy,ext)
chlomean<-crop(chlomean,ext)
ppmean<-crop(ppmean,ext)
tempmean<-crop(tempmean,ext)
odismean<-crop(odismean,ext)
salinity<-crop(salinity,ext)

writeRaster(bathy, filename="bathy.tif", format="GTiff", overwrite=TRUE)
writeRaster(chlomean, filename="chlomean.tif", format="GTiff", overwrite=TRUE)
writeRaster(ppmean, filename="ppmean.tif", format="GTiff", overwrite=TRUE)
writeRaster(odismean, filename="odismean.tif", format="GTiff", overwrite=TRUE)
writeRaster(salinity, filename="salinity.tif", format="GTiff", overwrite=TRUE)
writeRaster(tempmean, filename="tempmean.tif", format="GTiff", overwrite=TRUE)

# Ahora simplemente puedo cargarlos sin necesidad de descarga
files<-(list.files("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Predictores", full.names=T, pattern=".tif"))#change directory
predictors <- stack(files)
names(predictors) <- c("bathy","chlomean","ppmean","odismean","salinity","tempmean")
plot(predictors)

# Miro a ver si hay NAs
values(predictors) #Los hay y probablemente sean de la parte de la tierra

# Escalo los valores para que al hacer el análisis todas las variables tengan la misma escala 
predictors2 <- scale(predictors)
round(apply(values(predictors2), 2, summary), 4) #Me cercioro que estén con media en cero
```

#############################################################
##### --- Presencias, Pseudoausencias & Otras cosas --- #####
#############################################################

```{r}
# Presencias: 
# Extraigo las coordenadas y les doy nombre
coords=cbind(data$Lon, data$Lat) 
colnames(coords)<-c("x","y") 

# La función extract permite extraer los valores de la variable ambiental de las coordenadas donde están las presencias
presvals <- extract(predictors2, coords) 

# Me cercioro de que todo está correcto
head(presvals)
```

```{r}
# Pseudoausencias
# Pongo una semilla para que me aparezcan los puntos en el mismo sitio
# Genero 1000 pseudoausencias en la zona que me interesa
set.seed(141592) 
backgr <- randomPoints(predictors2, 1000) 

# Lo transformo en data.frame para poder trabajar con él
backgr <-as.data.frame(backgr) 
head(backgr)
```

```{r}
# Pinto las pseudoausencias con el fin de ver que efectivamente se han creado
plot(wrld_simpl, xlim= c(-25,40),ylim=c(30,65), axes=TRUE,col="light yellow")
points(data$Lon, data$Lat, col="orange", pch=20, cex=0.75)
points(backgr, col="black", pch=20, cex=0.75)
```


```{r}
# Extraigo valores de las variables ambientales de las pseudoausencias
absvals <- extract(predictors2, backgr) 

# Me cercioro de que todo está bien
head(absvals)
```

```{r}
#Hago La Base de Datos DEFINITIVA DEFINITIVÍSIMA
# Genero un único vector de coordenadas
coords<-as.data.frame(rbind(coords,backgr)) 

# Genero un vector con tantas presencias (1) y ausencias (0) haya
pb <- c(rep(1, nrow(presvals)), rep(0, nrow(absvals)))

# Junto los datos de las variables ambientales de presencia y pseudoausencia con los 0 y 1.
sdmdata <- data.frame(cbind(pb, rbind(presvals, absvals), coords)) 
head(sdmdata)
```

```{r}
#Miro si hay NAs y los quito si son pocos, pero si son muchos hago una imputación de la media de los datos que están cercanos.
summary(sdmdata) 
to.remove <- which(!complete.cases(sdmdata))
sdmdata <- sdmdata[-to.remove,]

# Vuelvo a hacer el summary para cerciorarme de que ya no tengo NAs
summary(sdmdata)

# Guardo la base de datos para usarla posteriormente.
write.csv(sdmdata,"Datos/sdmdata.csv")

# Cargo la base de datos
sdmdata <- read.csv("Datos/sdmdata.csv")
```

###############################
##### --- Descriptiva --- #####
###############################

```{r}
# Miro la correlación entre variables para identificar qué variables o no incluir.
# La X muestra que no hay correlación significativa.
# La correlación se empieza a plantear el no introducirlaa partir de 0.7.
# Hago modelos con correlación y sin ella para ver cómo ajustan y ver cómo se explica.

matrix<-rcorr(as.matrix(sdmdata[,c(2:10)]), type = "pearson")

# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}

corrplot(matrix$r, type="lower", tl.col = "black",method="number",
         p.mat = matrix$P, sig.level = 0.05)
```

Se puede observar que las correlaciones entre las distintas variables son bastante altas:
- pb (mi variable respuesta) y bathy, 0.53. No es exageradamente alta como otras que vamos a ver pero ahí está.
- Chlomean y odismean con un 0.89, bastante alta.
- Chlomean también correlaciona negativamente con tempmean -0.64. No supera por los pelos el 0.7 pero es ligeramente elevado.
- ppmean tiene una correlación negativa alta con salinidad -0.8 y tempmean -0.88.

A si a primera vista diría que las variables mas indicadas para entrar en el modelo serían: bathy, odismean, salinity y tempmean. 
También podría valorarse cambiar chlomean por odismean.

```{r}
# Paso 2
# Miro la relación entre la variable respuesta y las covariables con el fin de ver si tienen una relación lineal. 
# Este análisis lo necesitaré posteriormente para considerar o no hacer un gam
my_fn <- function(data, mapping, ...){
  p <- ggplot(data = data, mapping = mapping) + 
    geom_point() + 
    geom_smooth(method=loess, fill="red", color="red", ...) +
    geom_smooth(method=lm, fill="blue", color="blue", ...)
  p
}

g = ggpairs(sdmdata, lower = list(continuous = my_fn))
g
```

En este pairs se aprecia lo siguiente:
- No parece que niguna de las variables sea normal incluyendo la variable respuesta. Por lo que habrá que usar un GLMs o alternativas similares.
- La variable respuesta pb (presencias y ausencias) se distribuye como una binomial.

```{r}
# Compruebo si hay o no multicolinealidad entre variables
# El objetivo es quedarse con un conjunto de variables con un VIF menor a 5 y 3.

source("HighstatLib.r") #Función ya hecha
corvif(sdmdata[,c(1:9)])
```

Con respecto al GVIF hay una gran cantidad de variables que superan con creces el valor de 5 a excepción de batimetría que tiene nu GVIF de 2.44.
A priori no voy a eliminar variables a la hora de hacer modelos, sino que iré mirando poco a poco con qué variables se obtiene un mejor ajuste.

##################################
##### --- Ajuste modelos --- #####
##################################

# BIOCLIM

```{r}
# Construyo el modelo
bc.model <- bioclim(x = predictors2, p = data)
```

```{r}
# FUNCIÓN DEFINITIVA DEFINITIVÍSIMA MILENARIA XTREM
# Esta función sirve para hacer una cross validation y obtener así los distintos índices que me interesan

fddmx <- function(coordenadas, predictores,background){

      group <- kfold(coordenadas, 5)

      pres_train <- coordenadas[group != 1, ] #Las que no sean 1
      pres_test <- coordenadas[group == 1, ] #Las que sean 1
      
      bc <- bioclim(predictores, pres_train)

      group <- kfold(background, 5) #Hago 5 grupos de esos puntos
      backg_train <- background[group != 1, ]
      backg_test <- background[group == 1, ]
      
      eval.modesta <- evaluate(pres_test, backg_test, bc, predictores)
      
      auc_bc.model <- eval.modesta@auc #auc
      
      cor_bc.model <- eval.modesta@cor #cor
      
      kappa_bc.model <- mean(eval.modesta@kappa) #Kappa

      sensibility_bc.model <- mean(eval.modesta@TPR/(eval.modesta@TPR+eval.modesta@FNR)) #Sensibilidad
      
      specificity_bc.modelo <- mean(eval.modesta@TNR/(eval.modesta@FPR+eval.modesta@TNR)) #Especificidad
      
      TSSbc.model <- mean(eval.modesta@TPR+eval.modesta@TNR-1) #TSS
      
      return(c(auc_bc.model,cor_bc.model,kappa_bc.model,sensibility_bc.model,specificity_bc.modelo,TSSbc.model))
}


# Hago una matriz para guardar los valores de los coeficientes de las diez iteraciones que me devuelva la función 
cosites_bc.model <- matrix(ncol=6,nrow=10) 
      
# Genereo un bucle for de 10 iteraciones que me devuelva 10 valores diferentes de los 6 coeficientes.
for (i in 1:10) { cosites_bc.model[i,] <- fddmx(coords,predictors2,backgr)}

# Le doy nombre a las columnas para saber qué coeficientes hay dentro
colnames(cosites_bc.model) <- c('AUC','COR','Kappa','Sensitivity','Specificity','TSS')

# Hago la media por columnas que es lo que me interesa y lo guardo
coef_bc.model <- apply(cosites_bc.model, 2, mean)
write.csv(coef_bc.model,'BIOCLIM/coeficientes_BIOCLIM.csv')
```

```{r}
# Predicciones de presencia
predict.presence.bc.bioclim <- dismo::predict(object = bc.model, 
                                   x = predictors2, 
                                   ext = ext)

plot(predict.presence.bc.bioclim)

# Me guardo el modelo predictivo
saveRDS(predict.presence.bc.bioclim, "BIOCLIM/Predicciones_BIOCLIM.ascii")
```


En el mapa predictivo se puede ver que  en el Mar del Norte y en la parte de costa Oeste de Gran Bretaña y Francia hay una probabilidad de presencia de hasta el 0.8. Este valor va disminuyendo progresivamente a medida que se aleja de la zona.
Por otro lado, en la costa de España, Norte de Italia, sobre todo en la noreste, y costa africana cercana a la península ibérica la probabilidad de presencia es de 0.4-0.2.


.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
El modelo BIOCLIM no puede hacerse en bayesiano.
Tampoco incluye una componente espacial.

# MAXENT

Frecuentista: Video de youtube
Bayesiano: ¿?

```{r}
# download maxent.jar 3.3.3k, and place the file in the desired folder
# utils::download.file(url="https://raw.githubusercontent.com/mrmaxent/Maxent/master/ArchivedReleases/3.3.3k/maxent.jar",
#                      destfile=paste0(system.file("java", package="dismo"),"/maxent.jar"),
#                      mode="wb") ## wb for binary file, otherwise maxent.jar can not execute

# We create a buffer around our occurrence locations and define this as our study region, which will allow us to avoid sampling from a broad background.
# We establish a four-decimal-degree buffer around the occurrence points. 
# To make sure that our buffer encompasses the appropriate area, we plot the occurrence points, the first environmental layer, and the buffer polygon.

```

# GLM
Frecuentista: ¿?
Bayesiano: INLA


# RF
Frecuentista: Clase 1 María
Bayesiano: ¿?

```{r}
# No tiene en cuenta el error.
# 
# library(randomForest)
# attach(data)
# 
# coords<- cbind(data$Lon,data$Lat)
# colnames(coords)<- c('x','y')
# 
# backgr <- randomPoints(predictors2, 855)
# 
# presvals <- extract(predictors2, coords)  #Hay muchos NAs ¿Qué pasa?
# absvals <- extract(predictors2, backgr)
# 
# pb <- c(rep(1, nrow(presvals)), rep(0, nrow(absvals))) #1 donde hay presencia y 0 donde hay ausencias
# 
# 
# # INTENTO
# sdmdata <- data.frame(cbind(pb, rbind(presvals, absvals), coords)) 
# 
# model <- pb ~ calcite+nitrate+salinity
# rf3 <- randomForest(model, sdmdata);rf3
# varImpPlot(rf3)
# 
# #Now prediction
# prf= predict(predictors2, rf3,type="response") #Puedo hacer la predicción también
# 
# #tiff("Figure_RF.tiff", width = 3700, height = 1800, res = 300)
# par(mfrow=c(1,1))
# plot(prf)
# plot(wrld_simpl, axes=TRUE,add=T,col="grey")
# points(sdmdata$x,sdmdata$y, col=sdmdata$pb+1,pch=20, cex=0.75)
#dev.off()

```


```{r}
# Validación Cruzada
# model <- pb ~ calcite+nitrate+salinity
# mrf1 <- randomForest(model, traindata);mrf1
# 
# erf <- evaluate(testdata[testdata==1,], testdata[testdata==0,],mrf1);erf #Puedo hacer un cross validation
```

# BRT
Frecuentista: Clase 1 María
Bayesiano: ¿?

```{r}
# library(gbm)
# brt1 = gbm.step(data=sdmdata, gbm.x = c(2,3,4,5,6,7,8,9), gbm.y = 1, tree.complexity=1, family = "bernoulli",  learning.rate = 0.01)
# devexpl=((brt1$self.statistics$null-brt1$self.statistics$resid)/brt1$self.statistics$null)*100
# devexpl
# summary(brt1)
# 
# #Try without variables with low relevance (< 4% relative variance expaliend)
# brt2 = gbm.step(data=sdmdata, gbm.x = c(3,4,5,8), gbm.y = 1, tree.complexity=1, family = "bernoulli",  learning.rate = 0.01)
# #Los números es la posición de la variable dentro de la base de datos
# # gbm.step permite calcular brt con un número de decimales. Learning.rate = cuánto va aprendiendo para mejorar la inclusión de variables
# # Va indicando la proporción de deviance explicada del conjunto de variables 
# 
# devexpl=((brt2$self.statistics$null-brt2$self.statistics$resid)/brt2$self.statistics$null)*100
# devexpl #% de deviance explicada por el modelo
# summary(brt2) #% de deviance explicada por cada variable
# 
# #Try without variables with low relevance (< 4% relative variance expaliend)
# brt3 = gbm.step(data=sdmdata, gbm.x = c(2,4,8), gbm.y = 1, tree.complexity=1, family = "bernoulli",  learning.rate = 0.01)
# devexpl=((brt3$self.statistics$null-brt3$self.statistics$resid)/brt3$self.statistics$null)*100
# devexpl
# summary(brt3) #Se puede observar en el diagrama 
# 
# # Se está usando este tipo de modelos para hacer la selección de variables 
# 
# #Plot the functional responses 
# gbm.plot(brt3, n.plots=3, write.title=FALSE, plot.layout=c(1,3), common.scale=F) #respuesta funcional de la variable con respecto a la variable respuesta
# 
# #Now prediction
# pbrt= predict(predictors2, brt3,type="response", n.trees=brt3$n.trees, shrinkage= 0.01, distribution="bernoulli")
# # Con ese modelo se puede aplicar predict
# 
# #tiff("Figure_BRT.tiff", width = 3700, height = 1800, res = 300)
# par(mfrow=c(1,1))
# plot(pbrt)
# plot(wrld_simpl, axes=TRUE,add=T,col="grey") #Sale un mapa similar al glm pero este muestra mas movimiento, imagen mas heterogeneo
# points(sdmdata$x,sdmdata$y, col=sdmdata$pb+1,pch=20, cex=0.75)
# #dev.off()

```

```{r}
# Validación cruzada
# mbrt= gbm.step(data=traindata, gbm.x = c(2,4,8), gbm.y = 1, tree.complexity=1, family = "bernoulli",  learning.rate = 0.01)
# 
# ebrt <- evaluate(testdata[testdata==1,], testdata[testdata==0,],mbrt,n.trees=mbrt$n.trees);ebrt
```














