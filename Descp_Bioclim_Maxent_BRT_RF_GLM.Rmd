---
title: "Modelos (Descriptiva, BIOCLIM, MAXENT, GLM, BRT & RF)"
author: "Irene Extremera Serrano"
date: "11/2/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(eval = TRUE, warning = FALSE, error = FALSE, message = FALSE, comment = " ")
```

````{r, warning=FALSE, message=FALSE, }

library(sp) #Tabajar con objetos de tipo espacial
library(rgdal) #Es necesaria para la librería sdmpredictors
library(carData)
library(car)
library(nlme)
library(gstat)
library(sf)
library(spData)
library(spdep)
library(lattice)
library(survival)
library(Formula)
library(ggplot2) #Para gráficos de las predicciones
library(Hmisc)
library(raster) #Para poder trabajar con objetos tipo raster
library(leaflet)
library(GGally)
library(maptools)
library(corrplot)
library(rgeos)
library(maptools) #Cargar mapas
library(dismo) #Para poder trabajar con BIOCLIM y MAXENT
library(sdmpredictors) #Para descargarme las variables ambientales
library(PresenceAbsence)
library(rJava)
library(randomForest)
library(INLA) #Para trabajar con INLA
library(Matrix)
library(parallel)
library(foreach)
library(dotCall64)
library(grid)
library(spam)
library(fields)
library(randomForest) #Para realizar Random Fores
library(gbm) #Para realizar vosted regression tree

```

#################################################
########    CARGAR LAS BASES DE DATOS    ########
#################################################

```{r}

# Asignamos un directorio de trabajo

setwd("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Rmd")

# MAPA DEL MUNDO

data(wrld_simpl) 

# PRESENCIA DE ANCHOAS
# Descargué la base de datos de AQUAMAPS y eliminé las ubicaciones que no perteneciesen a Atlántico norte mediante la función: identify() y posteriormente quitándolas de la base de datos.
# Otra cosa a realizar fue la identificación de duplicados mediante las siguientes líneas de código:

# dups2 <- duplicated(data[, c("Lon","Lat")]) #Miro si hay duplicados
# sum(dups2) #Veo cuántos duplicados 
# data <- data[!dups2, ] #Verifico si son diferentes antes de eliminarlos.

# Si existiesen duplicados habría que eliminarlos de la base de datos, en este caso no hubo ninguno.
# Después de todo este procedimiento, guardé esa base de datos con presencias únicamente de Atlántico norte.

data <- read.csv('C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Datos/Anchoas_Aqua_atln.csv',TRUE,",")


data <- data[,-1] #Le quito al primera que es una columna de índices
colnames(data) <- c('Lon','Lat') #Le doy nombre a las columnas

#Realizo un plot para cerciorarme que todo está en orden 

plot(wrld_simpl, xlim= c(-20,35),ylim=c(43,70), axes=TRUE,col="light yellow", main='Presencias Anchoa')
points(data$Lon, data$Lat, col="blue", pch=20, cex=0.75)
points(data$Lon, data$Lat, col="pink", cex=0.75)

```

```{r}

#PREDICTORES de MASPEC y BIO-ORACLE
# A la hora de seleccionar variables nos hemos guiado por la biología de la especie. Por lo que decidimos trabajar con: batimetría, salinidad, oxígeno disuelto, temperatura media superficial, clorofila media y producción primaria media.

# Para descargarlas utilicé las siguientes líneas de código:

# Llamo primero a los rasters por separado. Como se puede observar, batimetría pertenece a MASPEC y el resto a Bio-Oracle.

# bathy<- load_layers(c("MS_bathy_5m"))
# chlomean<- load_layers(c("BO2_chlomean_ss"))
# ppmean<- load_layers(c("BO2_ppmean_ss"))
# tempmean <- load_layers(c("BO2_tempmean_ss"))
# odismean<- load_layers(c("BO2_dissoxmean_ss"))
# salinity <- load_layers(c("BO2_salinitymean_ss"))

# La longitud de bathy y chlomean es diferente por lo que había que igualarlas al resto con la función resample.

# bathy<-resample(bathy,salinity)
# chlomean<-resample(chlomean,salinity)

# ¿CÓMO ME DI CUENTA DE ESTO?

# Corté los rasters para luego guardarlos y que así me ocupasen menos 

# ext <-extent(-20,35,43,70) #Para Atlántico Norte
# bathy<-crop(bathy,ext)
# chlomean<-crop(chlomean,ext)
# ppmean<-crop(ppmean,ext)
# tempmean<-crop(tempmean,ext)
# odismean<-crop(odismean,ext)
# salinity<-crop(salinity,ext)

# writeRaster(bathy, filename="bathy_atln.tif", format="GTiff", overwrite=TRUE)
# writeRaster(chlomean, filename="chlomean_atln.tif", format="GTiff", overwrite=TRUE)
# writeRaster(ppmean, filename="ppmean_atln.tif", format="GTiff", overwrite=TRUE)
# writeRaster(odismean, filename="odismean_atln.tif", format="GTiff", overwrite=TRUE)
# writeRaster(salinity, filename="salinity_atln.tif", format="GTiff", overwrite=TRUE)
# writeRaster(tempmean, filename="tempmean_atln.tif", format="GTiff", overwrite=TRUE)

```

```{r}

# Ahora simplemente pueden cargarse sin necesidad de tener que descargarlos.
# Para que salga bien es muy importante que los predictores estén en una misma carpeta.

files<-(list.files("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Predictores/atl", full.names=T, pattern=".tif"))#change directory
predictors <- stack(files)
names(predictors) <- c("bathy","chlomean","ppmean","odismean","salinity","tempmean")
plot(predictors) #Los pinto

```

```{r}

# Miro a ver si hay NAs
# values(predictors) #Los hay y probablemente sean de la parte de la tierra

# Escalo los valores para que al hacer el análisis todas las variables tengan la misma escala 

predictors2 <- scale(predictors)
round(apply(values(predictors2), 2, summary), 4) #Me cercioro que estén con media en cero

# Para comprobar que todo está igual para que no me dé problemas habría que usar las siguientes funciones:
# crs() #miramos el systema de referencia de las coordenadas #+proj=longlat +datum=WGS84 +no_defs 
# res() #ver resolucion del raster #0.08333333 0.08333333
# yres() ; xres() #ver resolucion del raster

```

El sistema de referencia es el de coordenadas cartográficas y la resolución es de 0.0833º.

#############################################################
##### --- Presencias, Pseudoausencias & Otras cosas --- #####
#############################################################

```{r}

# Una vez que se dispone de las presencias y de las variables predictoras el objetivo siguiuente es: 
# 1- La creación de pseudoausencias
# 2- La adjudicación a esos puntos concretos de presencia y de pseudoausencia sus valores ambientales 
# 3- El almacenamiento de esta información en una base de datos.

# PRESENCIAS: 
# Extraigo las coordenadas de presencias y les doy nombre
# coords_pres <- cbind(data$Lon, data$Lat)
# colnames(coords_pres)<-c("x","y")

# La función extract permite extraer los valores de la variable ambiental de las coordenadas donde están las presencias
# presvals <- extract(predictors2, coords_pres)

# Me cercioro de que todo está correcto
# head(presvals)

```

```{r}

# PSEUDOAUSENCIAS
# Pongo una semilla para que me aparezcan los puntos en el mismo sitio
# Genero 1000 pseudoausencias en la zona que me interesa

set.seed(141592) 
backgr <- randomPoints(predictors2, 1000) 

# Lo transformo en data.frame para poder trabajar con ellas
# backgr <-as.data.frame(backgr)
# head(backgr) # Miro que todo está bien.

# Extraigo valores de las variables ambientales de las pseudoausencias
# absvals <- extract(predictors2, backgr)

Me cercioro de que todo está bien
# head(absvals)

```

```{r}

# Pinto las pseudoausencias con el fin de ver que efectivamente se han creado bien

plot(wrld_simpl, xlim= c(-20,35),ylim=c(43,70), axes=TRUE,col="light yellow")
points(data$Lon, data$Lat, col="pink", pch=20, cex=0.75)
points(backgr, col="black", pch=20, cex=0.75)

```


```{r}

# JUNTAR TODO EN UNA BASE DE DATOS
# Genero un único vector de coordenadas de presencias y pseudoausencias
# coords<-as.data.frame(rbind(coords,backgr))

# Genero un vector con tantas presencias (1) y ausencias (0) haya
# pb <- c(rep(1, nrow(presvals)), rep(0, nrow(absvals)))

# Junto los datos de las variables ambientales de presencia y pseudoausencia con los 0 y 1.
# sdmdata <- data.frame(cbind(pb, rbind(presvals, absvals), coords))
# head(sdmdata)

#Hay que mirar si hay NAs: si son pocos se quitan, pero si son muchos habría que valorar el hacer una imputación de la media de los datos que están cercanos.
# summary(sdmdata) 
# to.remove <- which(!complete.cases(sdmdata))
# sdmdata <- sdmdata[-to.remove,]

# Se vuelve a hacer el summary para cerciorarse de que ya no hay NAs
# summary(sdmdata)

# Se guarda la base de datos
# write.csv(sdmdata,"Datos/sdmdata.csv")

# Se carga la base de datos
sdmdata <- read.csv("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Datos/sdmdata.csv")

```


#############################################################
################## ----- Descriptiva ----- ##################
#############################################################

Una vez obtenidae la base datos a punto comienza la descriptiva.

```{r}

# Miro la correlación entre variables para identificar qué variables o no incluir.
# La correlación se empieza a plantear el no introducir la variable partir de 0.7.
# Hacer modelos con correlación y sin ella para ver cómo ajustan y ver cómo se explica (opcional)

matrix<-rcorr(as.matrix(sdmdata[,c(3:8)]), type = "pearson")

# ... : further arguments to pass to the native R cor.test function
cor.mtest <- function(mat, ...) {
  mat <- as.matrix(mat)
  n <- ncol(mat)
  p.mat<- matrix(NA, n, n)
  diag(p.mat) <- 0
  for (i in 1:(n - 1)) {
    for (j in (i + 1):n) {
      tmp <- cor.test(mat[, i], mat[, j], ...)
      p.mat[i, j] <- p.mat[j, i] <- tmp$p.value
    }
  }
  colnames(p.mat) <- rownames(p.mat) <- colnames(mat)
  p.mat
}

corrplot(matrix$r, type="lower", tl.col = "black",method="number",
         p.mat = matrix$P, sig.level = 0.05)

```

Se puede observar que las correlaciones entre algunas de las distintas variables son bastante altas:
- Chlomean y odismean con un 0.89, bastante alta y positiva.
- ppmean tiene una correlación negativa alta con salinidad -0.81 y tempmean -0.88.

A si a primera vista diría que las variables mas indicadas para entrar en el modelo serían: bathy, odismean, salinity y tempmean. 

A continuación realizo un ggpairs para ver qué tipo de relación hay entre las covariables escaladas y sin escalar.

```{r, echo=FALSE}

# Escaladas

par(mfrow=c(2,1))
attach(sdmdata)
G1<-ggplot(sdmdata, aes(bathy,chlomean) ) +
  geom_point()
G1

G2<-ggplot(sdmdata, aes(bathy,ppmean) ) +
  geom_point()
G2

G3<-ggplot(sdmdata, aes(bathy,salinity) ) +
  geom_point()
G3

G4<-ggplot(sdmdata, aes(bathy,odismean) ) +
  geom_point()
G4


G5<-ggplot(sdmdata, aes(bathy,tempmean) ) +
  geom_point()
G5


G6<-ggplot(sdmdata, aes(ppmean,chlomean) ) +
  geom_point()
G6

G7<-ggplot(sdmdata, aes(ppmean,odismean) ) +
  geom_point()
G7

G8<-ggplot(sdmdata, aes(ppmean,tempmean) ) +
  geom_point()
G8

G9<-ggplot(sdmdata, aes(ppmean,salinity) ) +
  geom_point()
G9

G10<-ggplot(sdmdata, aes(salinity,chlomean) ) +
  geom_point()
G10

G11<-ggplot(sdmdata, aes(salinity,tempmean) ) +
  geom_point()
G11

G12<-ggplot(sdmdata, aes(salinity,odismean) ) +
  geom_point()
G12

G13<-ggplot(sdmdata, aes(odismean,tempmean) ) +
  geom_point()
G13

G14<-ggplot(sdmdata, aes(odismean,chlomean) ) +
  geom_point()
G14

G15<-ggplot(sdmdata, aes(tempmean,chlomean) ) +
  geom_point()
G15

```

Se observa que la forma que tienen de relacionarse las variables entre ellas es bastante extraña en algunos casos. Por ello se valora a continuación realizar estos mismos gráficos con las variables ambientales sin escalar.


```{r}

# SIN ESCALAR
# Para ello hay que preparar una base de datos como la de sdmdata pero con los predictores no escalados

# PRESENCIAS

coords_pres <- cbind(data$Lon, data$Lat)
colnames(coords_pres)<-c("x","y")
presvals_nos <- extract(predictors, coords_pres)

# AUSENCIAS

absvals_nos <- extract(predictors, backgr)

# COORDENADAS

coords_nos<-as.data.frame(rbind(coords_pres,backgr))

# 1 & 0

pb <- c(rep(1, nrow(presvals_nos)), rep(0, nrow(absvals_nos)))

# BASE DE DATOS

sdmdata_nos <- data.frame(cbind(pb, rbind(presvals_nos, absvals_nos), coords_nos))

# NAs

to.remove <- which(!complete.cases(sdmdata_nos))
sdmdata_nos <- sdmdata_nos[-to.remove,]

```


```{r}

#Si ya los tienes descargados y guardados

files<-(list.files("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Predictores/atl/Seleccionadas", full.names=T, pattern=".tif"))
predictors <- stack(files)
names(predictors) <- c("bathy","odismean","salinity","tempmean")

# Escalo los valores para que al hacer el análisis todas las variables tengan la misma escala 

predictors3 <- scale(predictors) #Ya he comprobado que tienen su media en cero en otro script

# Pseudoausencias
# Pongo una semilla para que me aparezcan los puntos en el mismo sitio
# Genero 1000 pseudoausencias en la zona que me interesa

files<-(list.files("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Predictores/atl", full.names=T, pattern=".tif"))
predictors <- stack(files)
names(predictors) <- c("bathy","chlomean","ppmean","odismean","salinity","tempmean")
predictors2 <- scale(predictors)
set.seed(141592) 
backgr <- randomPoints(predictors2, 1000) 

# Lo transformo en data.frame para poder trabajar con él

backgr <-as.data.frame(backgr) #Incluyo esta linea de código pero estos valores están dentro de sdmdata

# Cargo la base de datos

sdmdata <- read.csv("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Datos/sdmdata_atl.csv")
```

```{r, echo=FALSE}

# Escaladas

par(mfrow=c(2,1))
attach(sdmdata_nos)
G1<-ggplot(sdmdata_nos, aes(bathy,chlomean) ) +
  geom_point()
G1

G2<-ggplot(sdmdata_nos, aes(bathy,ppmean) ) +
  geom_point()
G2

G3<-ggplot(sdmdata_nos, aes(bathy,salinity) ) +
  geom_point()
G3

G4<-ggplot(sdmdata_nos, aes(bathy,odismean) ) +
  geom_point()
G4


G5<-ggplot(sdmdata_nos, aes(bathy,tempmean) ) +
  geom_point()
G5


G6<-ggplot(sdmdata_nos, aes(ppmean,chlomean) ) +
  geom_point()
G6

G7<-ggplot(sdmdata_nos, aes(ppmean,odismean) ) +
  geom_point()
G7

G8<-ggplot(sdmdata_nos, aes(ppmean,tempmean) ) +
  geom_point()
G8

G9<-ggplot(sdmdata_nos, aes(ppmean,salinity) ) +
  geom_point()
G9

G10<-ggplot(sdmdata_nos, aes(salinity,chlomean) ) +
  geom_point()
G10

G11<-ggplot(sdmdata_nos, aes(salinity,tempmean) ) +
  geom_point()
G11

G12<-ggplot(sdmdata_nos, aes(salinity,odismean) ) +
  geom_point()
G12

G13<-ggplot(sdmdata_nos, aes(odismean,tempmean) ) +
  geom_point()
G13

G14<-ggplot(sdmdata_nos, aes(odismean,chlomean) ) +
  geom_point()
G14

G15<-ggplot(sdmdata_nos, aes(tempmean,chlomean) ) +
  geom_point()
G15

```

Al comparar las gráficas en las que se relacionan las variables escaladas y no se comprueba que efectivamente entre ellas tienen una relación un poco extraña. 

Estas relaciones entre las covariables sirve de aliciente para pensar que uno de los modelos a realizar sea un modelo aditivo generalizado (GAM).
Además, cabe mencionar que la variable respuesta se distribuye como una binomial por lo que otro de los modelos a plantear será un modelo lineal generalizado (GLM).

En la linea de selección de variables, a continuación se mirará la multicolinealidad entre ellas mediante el VIF (factor de inflación de la varianza). Para ello se partirá de todas las variables y posteriormente se irán eliminando una a una las que tengan un mayor valor hasta que el grupo quede con valores de VIF menores a 3.

```{r}

# Compruebo si hay o no multicolinealidad entre variables
# El objetivo es quedarse con un conjunto de variables con un VIF menor a 5 y 3.

source("HighstatLib.r") #Función ya hecha
# corvif(sdmdata[,c(3:8)]) #Solo la hago para las que no son covaraibles

corvif(sdmdata[,c(3,6,7,8)]) #He ido quitando una a una las que tenían un GVIF mas alto, ppmean y chlmean

```

Las variables con las que me quedaré finalmente han sido: batimetría, oxígeno disuelto, temperatura media y salinidad. 

```{r}

# Creo una variable predictores que solo incluya a esas cuatro
# Solo uso las covariables seleccionadas

cuatro_pred <- (list.files("C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/Predictores/atl/Seleccionadas", full.names=T, pattern=".tif")) #Cargo bathy que lo he metido en una carpeta aparte
predictors <- stack(cuatro_pred)
names(predictors) <- c("bathy","odismean","salinity","tempmean")
predictors3 <- scale(predictors)

# Elimino esas covariables de la base de datos

sdmdata <- sdmdata[,c(-1,-4,-5)]

```

##################################
##### --- Ajuste modelos --- #####
##################################

El esquema que voy a intentar seguir en los modelos va a ser el siguiente:

- Ajuste
- Valoración del modelo usando los residuos (Si puede hacerse)
- Predictiva
- Plot
- Validación cruzada
- Índice de Moran para ver si puede meterse la componente espacial


##### --- BIOCLIM --- #####

Primero comenzaré con el modelo mas sencillo, bioclim del paquete dismo.

```{r}

# Hago el modelo de BIOCLIM

bc.model <- bioclim(x = predictors3, p = data)
bc.model

# Guardo el modelo

saveRDS(bc.model, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/BIOCLIM/bc.model.rds") #Guardo el modelo BIOCLIM

```

Dentro del objeto no hay residuos por lo que no puedo hacer ningún tipo de análisis de los residuos. Por lo que paso directamente a la realización y representación de la predictiva.

```{r}

# Predictiva de presencias
ext <-extent(-20,35,43,70) #Para Atlántico Norte

predict_presence_bc_bioclim <- predict(object = bc.model,
                                   x = predictors3,
                                   ext = ext)

# La pinto
ggplot() +
  geom_raster(data = raster::as.data.frame(predict_presence_bc_bioclim , xy = TRUE) , aes(x = x, y = y, fill = layer)) +
  coord_equal() +
  labs( x = "", y = "")+theme_minimal()+
  scale_color_brewer(palette = 'YlGnBu')+ labs(fill='') +
  guides(fill = guide_legend(label.position = "left", label.hjust = 1))

# La guardo
saveRDS(predict_presence_bc_bioclim, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/BIOCLIM/Predicciones_BIOCLIM_Atl.ascii")

```

En el mapa predictivo se puede ver que  en el Mar del Norte y en la parte de costa Oeste de Gran Bretaña y Francia hay una probabilidad de presencia de hasta el 0.8. Este valor va disminuyendo progresivamente a medida que se aleja de la zona.

A continuación valoro la calidad predictiva del modelo mediante la media de diez validaciones cruzadas.

```{r}

# FUNCIÓN DEFINITIVA DEFINITIVÍSIMA MILENARIA XTREM para BIOCLIM.
# Esta función sirve para hacer una cross validation y obtener así los distintos índices que me interesan.

fddmx <- function(coordenadas, predictores,background){

      group <- kfold(coordenadas, 5)

      pres_train <- coordenadas[group != 1, ] #Las que no sean 1
      pres_test <- coordenadas[group == 1, ] #Las que sean 1
      
      bc <- bioclim(predictores, pres_train)

      group <- kfold(background, 5) #Hago 5 grupos de esos puntos
      backg_train <- background[group != 1, ]
      backg_test <- background[group == 1, ]
      
      eval.modesta <- evaluate(pres_test, backg_test, bc, predictores)
      
      auc_model <- eval.modesta@auc #auc
      
      cor_model <- eval.modesta@cor #cor
      
      kappa_model <- mean(eval.modesta@kappa) #Kappa

      sensibility_model <- mean(eval.modesta@TPR/(eval.modesta@TPR+eval.modesta@FNR)) #Sensibilidad
      
      specificity_modelo <- mean(eval.modesta@TNR/(eval.modesta@FPR+eval.modesta@TNR)) #Especificidad
      
      TSS_model <- mean(eval.modesta@TPR+eval.modesta@TNR-1) #TSS
      
      return(c(auc_model,cor_model,kappa_model,sensibility_model,specificity_modelo,TSS_model))
}

# Hago una matriz para guardar los valores de los coeficientes de las diez iteraciones que me devuelva la función 
cosites_bc_model <- matrix(ncol=6,nrow=10) 
      
# Genereo un bucle for de 10 iteraciones que me devuelva 10 valores diferentes de los 6 coeficientes.
for (i in 1:10) { cosites_bc_model[i,] <- fddmx(data,predictors3,backgr)}

# Le doy nombre a las columnas para saber qué coeficientes hay dentro
colnames(cosites_bc_model) <- c('AUC','COR','Kappa','Sensitivity','Specificity','TSS')

# Hago la media por columnas que es lo que me interesa y lo guardo
coef_bc_model <- apply(cosites_bc_model, 2, mean)
write.csv(coef_bc_model,'C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/BIOCLIM/coeficientes_BIOCLIM.csv')#Guardo los coeficientes
coef_bc_model

```

Como el modelo de bioclim del paquete dismo no da residuos no puedo utilizarlos para valorar si existe correlación espacial usando el índice de Moran.

Por lo que esto es todo lo que podría realizarse con este modelo (?)


##### --- MAXENT --- #####

El siguiente modelo a realizar será MAXENT, que es muy sencillo de realizar computacionalmente pero es difícil de interpretar en cuanto a lo que hace por dentro (caja negra).

```{r}

# Construyo el modelo

maxent_model <- maxent(predictors3, data,backgr)
saveRDS(maxent_model, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/MAXENT/maxent_model_atl.rds")

plot(maxent_model) 

```

En la gráfica Variable Contribution, se está midiendo el porcentaje de cada variable en cuando a cómo es de importante a la hora de ajustar el modelo. Según muestra la gráfica, las variables mas importantes son la batimetría (70%) y la temperatura media superficial (28%). Siendo salinidad la tercera que mas contribuye con un 2% y odismean con un 0%.

Como el modelo MAXENT del paquete dismo no da tampoco los residuos, no puedo hacer un análisis de los mismo, por lo que pasaré a la realización de la predicción.

```{r}

# Predicciones de presencia

predict_presence_maxent_model <- dismo::predict(object = maxent_model, 
                                   x = predictors3, 
                                   ext = ext)
# La pinto
ggplot() +
  geom_raster(data = raster::as.data.frame(predict_presence_maxent_model , xy = TRUE) , aes(x = x, y = y, fill = layer)) +
  coord_equal() +
  labs( x = "", y = "")+theme_minimal()+
  scale_color_brewer(palette = 'YlGnBu')+ labs(fill='')+

# Me guardo el modelo predictivo
saveRDS(predict_presence_maxent_model, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/MAXENT/Predicciones_MAXENT_alt.ascii")

```

Al igual que mostraba BIOCLIM, hay una alta probabilidad de ocurrencia alrededor de la costa de Gran Bretaña y Francia. Sin embargo, en este modelo se aprecia que prácticamente en toda la costa europea hay una alta probabilidad de presencia a excepción de en el interior del mar Blanco y la costa este y oeste de Noruega.

```{r}

fddmx1 <- function(coordenadas, predictores,background){

      group <- kfold(coordenadas, 5)

      pres_train <- coordenadas[group != 1, ] #Las que no sean 1
      pres_test <- coordenadas[group == 1, ] #Las que sean 1
      
      me_m<- maxent(predictores, pres_train,background)

      group <- kfold(background, 5) #Hago 5 grupos de esos puntos
      backg_train <- background[group != 1, ]
      backg_test <- background[group == 1, ]
      
      eval.modesta <- evaluate(pres_test, backg_test, me_m, predictores)
      
      auc_model <- eval.modesta@auc #auc
      
      cor_model <- eval.modesta@cor #cor
      
      kappa_model <- mean(eval.modesta@kappa) #Kappa

      sensibility_model <- mean(eval.modesta@TPR/(eval.modesta@TPR+eval.modesta@FNR)) #Sensibilidad
      
      specificity_modelo <- mean(eval.modesta@TNR/(eval.modesta@FPR+eval.modesta@TNR)) #Especificidad
      
      TSS_model <- mean(eval.modesta@TPR+eval.modesta@TNR-1) #TSS
      
      return(c(auc_model,cor_model,kappa_model,sensibility_model,specificity_modelo,TSS_model))
}

# Hago una matriz para guardar los valores de los coeficientes de las diez iteraciones que me devuelva la función 
cosites_maxent_model <- matrix(ncol=6,nrow=10) 
      
# Genereo un bucle for de 10 iteraciones que me devuelva 10 valores diferentes de los 6 coeficientes.
for (i in 1:10) { cosites_maxent_model[i,] <- fddmx1(data,predictors3,backgr)}

# Le doy nombre a las columnas para saber qué coeficientes hay dentro
colnames(cosites_maxent_model) <- c('AUC','COR','Kappa','Sensitivity','Specificity','TSS')


# Hago la media por columnas que es lo que me interesa y lo guardo
coef_mx_model <- apply(cosites_maxent_model, 2, mean)
coef_mx_model 
write.csv(coef_mx_model,'C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/MAXENT/coeficientes_MAXENT_atln.csv')
```

Al igual que el anterior, al no disponer de los residuos no se puede ver si hay correlación espacial entre las observaciones usando el índice de Moran.


##### --- BOOSTED REGRESSION TREES --- #####

```{r}

# Modelo

brt1 <-  gbm.step(data=sdmdata, gbm.x = c(2,3,4,5), gbm.y = 1, tree.complexity=1, family = "bernoulli",  learning.rate = 0.01)

saveRDS(brt1, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT/brt_model.rds")

```
(No es necesario interpretar e incluir la gráfica de holdout deviance en el TFM)

```{r}

# Análisis del modelo
devexpl <- ((brt1$self.statistics$null-brt1$self.statistics$resid)/brt1$self.statistics$null)*100
devexpl #% de deviance explicada por el modelo
summary(brt1) #% de deviance explicada por cada variable
```

44.76% de deviance es explicada por el modelo.

Además. aproximadamente un 65% de la deviance es explicada por batimetría y cerca del 25% por parte de la temperatura media. En menor medida, 10% de lo que queda es explicado por las dos variables que quedan.

```{r}

# Plot de la respuesta funcional de la variable explicativa con respecto a la respuesta

gbm.plot(brt1, n.plots=3, write.title=FALSE, plot.layout=c(1,3), common.scale=F) 

```
¿LO COMENTO? ¿ES NECESARIO INCLUIRLO?

```{r}
# Predicción
pbrt <- predict(predictors3, brt1,type="response", n.trees=brt1$n.trees, shrinkage= 0.01, distribution="bernoulli")

# Representación
ggplot() +
  geom_raster(data = raster::as.data.frame(pbrt , xy = TRUE) , aes(x = x, y = y, fill = layer)) +
  coord_equal() +
  labs( x = "", y = "")+theme_minimal()+
  scale_color_brewer(palette = 'YlGnBu')+ labs(fill='')

saveRDS(pbrt , "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT/Predicciones_brt_Atl.ascii")

```

Se aprecia que hay una gran probabilidad de presencia cerca de la costa, en especial alrededor de Reino Unido e Irlanda.

```{r, results= "hide" }
# Validación Cruzada
fddmx3 <- function(coordenadas, base_datos,background){

      group <- kfold(coordenadas, 5) #kfold(1:1592, 5)

      pres_train <- base_datos[group != 1, ] #Las que no sean 1
      pres_test <- base_datos[group == 1, ] #Las que sean 1
      
      model <- gbm.step(data=pres_train, gbm.x = c(2,3,4,5), gbm.y = 1, tree.complexity=1, family = "bernoulli",  learning.rate = 0.01)

      group <- kfold(background, 5) #Hago 5 grupos de esos puntos
      backg_train <- background[group != 1, ]
      backg_test <- background[group == 1, ]
      
      eval.modesta <- evaluate(pres_test[pres_test==1,],pres_test[pres_test==0,],model)
      
      auc_modelo.model <- eval.modesta@auc #auc
      
      cor_modelo.model <- eval.modesta@cor #cor
      
      kappa_modelo.model <- mean(eval.modesta@kappa) #Kappa

      sensibility_modelo.model <- mean(eval.modesta@TPR/(eval.modesta@TPR+eval.modesta@FNR)) #Sensibilidad
      
      specificity_modelo.modelo <- mean(eval.modesta@TNR/(eval.modesta@FPR+eval.modesta@TNR)) #Especificidad
      
      TSS_modelo.model <- mean(eval.modesta@TPR+eval.modesta@TNR-1) #TSS
      
      return(c(auc_modelo.model,cor_modelo.model,kappa_modelo.model,sensibility_modelo.model,specificity_modelo.modelo,TSS_modelo.model))
}

# Hago una matriz para guardar los valores de los coeficientes de las diez iteraciones que me devuelva la función 
cosites_brt <- matrix(ncol=6,nrow=10) 

# Genereo un bucle for de 10 iteraciones que me devuelva 10 valores diferentes de los 6 coeficientes.
for (i in 1:10) {cosites_brt[i,] <- fddmx3(sdmdata[,c(6,7)],sdmdata,backgr)}

# Le doy nombre a las columnas para saber qué coeficientes hay dentro
colnames(cosites_brt) <- c('AUC','COR','Kappa','Sensitivity','Specificity','TSS')

# Hago la media por columnas que es lo que me interesa y lo guardo
coef_brt_model <- apply(cosites_brt, 2, mean) 

# Hago la media por columnas que es lo que me interesa y lo guardo
write.csv(coef_brt_model,'C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT//coeficientes_brt.csv')
coef_brt <- read.csv('C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT//coeficientes_brt.csv')
```

```{r}

coef_brt_model

```


```{r}

# CORRELACIÓN ESPACIAL
#Hago la matriz de vecinos para calcular el índice de Moran

nb <- dnearneigh(as.matrix(sdmdata[,c(6,7)]), 1,max(sdmdata[,c(6,7)]));nb 
listw <- nb2listw(nb,style = "S")

```

```{r}

MoranI <- moran.test(residuals(brt1), listw=listw, randomisation=FALSE); MoranI #p valor de 1 indica que Ho no se rechaza, no hay autocorrelación espacial

Moran_MC <- moran.mc(residuals(brt1), listw=listw, nsim=100); Moran_MC #p valor de 0.9901 Ho no se rechaza, lo cual indica que no hay autocorrelación espacial

```

Según el ínidce de Moran no se aprecia autocorrelación espacial, lo cual puede ser debido a que estoy trabajando con un area muy grande.


##### --- RANDOM FOREST --- #####

```{r}

# Modelo
model <- pb ~ bathy+salinity+tempmean+odismean
rf1 <- randomForest(model, sdmdata);rf1

saveRDS(rf1, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT//RF_model.rds")

```

Se puede apreciar que el porcentaje de deviance es de 39.83%. Un porcentaje menor en comparación a BRT.

```{r}

varImpPlot(rf1)

```

Es un diagrama que representa lo importante que son las variables a la hora de realizar el Random Forest.

```{r}

# Predicción

pred_rf <- predict(predictors3, rf1,type="response") #Puedo hacer la predicción también

# rf.resid <- rf1$predicted - sdmdata$pb #calculate residual

saveRDS(pred_rf , "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT//Predicciones_RF_Atl.ascii")

ggplot() +
  geom_raster(data = raster::as.data.frame(pred_rf , xy = TRUE) , aes(x = x, y = y, fill = layer)) +
  coord_equal() +
  labs( x = "", y = "")+theme_minimal()+
  scale_color_brewer(palette = 'YlGnBu')+ labs(fill='')

```

El mapa anterior tiene un aspecto muy similar al de BRT sin embargo, la pobabilidad de presencia parece difuminarse considerablemente en comparación.

```{r}

# Validación Cruzada
fddmx4 <- function(coordenadas, base_datos ,background,modelo){

      group <- kfold(coordenadas, 5) #kfold(1:1592, 5)

      pres_train <- base_datos[group != 1, ] #Las que no sean 1
      pres_test <- base_datos[group == 1, ] #Las que sean 1
      
      model <- randomForest(modelo, pres_train)

      group <- kfold(background, 5) #Hago 5 grupos de esos puntos
      backg_train <- background[group != 1, ]
      backg_test <- background[group == 1, ]
      
      eval.modesta <- evaluate(pres_test[pres_test==1,],pres_test[pres_test==0,],model)
      
      auc_modelo.model <- eval.modesta@auc #auc
      
      cor_modelo.model <- eval.modesta@cor #cor
      
      kappa_modelo.model <- mean(eval.modesta@kappa) #Kappa

      sensibility_modelo.model <- mean(eval.modesta@TPR/(eval.modesta@TPR+eval.modesta@FNR)) #Sensibilidad
      
      specificity_modelo.modelo <- mean(eval.modesta@TNR/(eval.modesta@FPR+eval.modesta@TNR)) #Especificidad
      
      TSS_modelo.model <- mean(eval.modesta@TPR+eval.modesta@TNR-1) #TSS
      
      return(c(auc_modelo.model,cor_modelo.model,kappa_modelo.model,sensibility_modelo.model,specificity_modelo.modelo,TSS_modelo.model))
}

# Hago una matriz para guardar los valores de los coeficientes de las diez iteraciones que me devuelva la función 
cosites_rf <- matrix(ncol=6,nrow=10) 

# Genereo un bucle for de 10 iteraciones que me devuelva 10 valores diferentes de los 6 coeficientes.
for (i in 1:10) {cosites_rf[i,] <- fddmx4(sdmdata[,c(6,7)],sdmdata,backgr,model)}

# Le doy nombre a las columnas para saber qué coeficientes hay dentro
colnames(cosites_rf) <- c('AUC','COR','Kappa','Sensitivity','Specificity','TSS')

# Hago la media por columnas que es lo que me interesa y lo guardo
coef_rf_model <- apply(cosites_rf, 2, mean);coef_rf_model 

# Hago la media por columnas que es lo que me interesa y lo guardo
write.csv(coef_rf_model,'C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT//coeficientes_rf.csv')
coef_glm <- read.csv('C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/RF y BRT//coeficientes_rf.csv')

```

```{r}

####### AUTOCORRELACIÓN ESPACIAL #######
# ¿¿¿???

```


##### --- MODELO LINEAL GENERALIZADO --- #####

Debido a que la variable respuesta se distribuye como una binomial voy a realizar un GLM introduciendo las covariables de forma aditiva.


```{r}

# GLM
glm1 <- glm(pb ~ bathy + odismean + tempmean + salinity, data=sdmdata, family="binomial") 

saveRDS(glm1, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/GLM frecuentista//GLM_model.rds")

# Resumen del modelo
summary(glm1) #Todas las variables salen significativas

```
Compruebo a continuación si hay colinealidad entre ellas.

```{r}

# Colinealidad

vif(glm1) #No la hay

```

```{r}

# VALORACIÓN DEL MODELO:
# Normalidad y homocedasticidad

par(mfrow=c(2,2))
plot(glm1) #No se ve normalidad ni homocedasticidad en los residuos

```

Los residuos no son normales, obviamente porque la variable respuesta es una binomial.

```{r}

res_m1 <- residuals(glm1,type='deviance')
shapiro.test(res_m1) #No hay normalidad p valor de 2.2e-16
1-pchisq(glm1$deviance, df = glm1$df.residual, lower.tail = F) #0.0168 hay diferencias significativas entre el modelo nulo y el ajustado

```

El shapiro test indica lo que ya se veia en los residuos, que no hay normalidad en ellos.
Con el pchisq de 0.246 se observa que no hay diferencias significativas entre el modelo nulo y el ajustado.

```{r}

# PREDICCIÓN
predm1<-predict(predictors3,glm1,type='response')

saveRDS(predm1, "C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/GLM frecuentista//Predicciones_glm_Atl.ascii")

ggplot() +
  geom_raster(data = raster::as.data.frame(predm1 , xy = TRUE) , aes(x = x, y = y, fill = layer)) +
  coord_equal() +
  labs( x = "", y = "")+theme_minimal()+
  scale_color_brewer(palette = 'YlGnBu')+ labs(fill='')

```

En este mapa se aprecia que la probabilidad de presencia es menor en comparación al resto pero parece estar bastante extendida por por la mayor parte de la costa Europea, Mar del Norte y un poco en el Mar Báltico.

```{r}

coords <- as.data.frame(cbind(sdmdata$x,sdmdata$y)) #Extraigo la lat y long de los puntos obs
ppm1<-extract(predm1,coords) #Lo mismo con los valores de probabilidad de los puntos observados
ppm1<-as.data.frame(ppm1) #Paso a data.frame
sdmdata1<-cbind(sdmdata,ppm1) #Lo junto todo a una base de datos
sdmdata1<-na.omit(sdmdata1) #Elimino los NAs
cor(sdmdata1$ppm1,sdmdata1$pb,method="spearman") # 0.518 es la correlación entre lo predicho y lo observado

plot(glm1$fitted.values, as.vector(unlist(ppm1)), main='Ajustados VS Observados', xlab='Valores Ajustados', ylab='Valores Observados',col=c('blue','green'))
abline(0,1)


# length(as.vector(unlist(ppm1))) #Había que quitar ppm1 de la lista para poder introducirlo

```
La correlación entre lo predicho y observado es del 0.49. ¿?


```{r}

# FUNCIÓN DEFINITIVA DEFINITIVÍSIMA MILENARIA XTREM: Funciona para glm.
# Esta función sirve para hacer una cross validation y obtener así los distintos criterios de valoración del modelo que me interesan

fddmx2 <- function(coordenadas, base_datos,background,model){

      group <- kfold(coordenadas, 5) #kfold(1:1592, 5)

      pres_train <- base_datos[group != 1, ] #Las que no sean 1
      pres_test <- base_datos[group == 1, ] #Las que sean 1
      
      modelo <- update(model, data = pres_train)

      group <- kfold(background, 5) #Hago 5 grupos de esos puntos
      backg_train <- background[group != 1, ]
      backg_test <- background[group == 1, ]
      
      eval.modesta <- evaluate(pres_test[pres_test==1,],pres_test[pres_test==0,],modelo)

      auc_modelo.model <- eval.modesta@auc #auc
      
      cor_modelo.model <- eval.modesta@cor #cor
      
      kappa_modelo.model <- mean(eval.modesta@kappa) #Kappa

      sensibility_modelo.model <- mean(eval.modesta@TPR/(eval.modesta@TPR+eval.modesta@FNR)) #Sensibilidad
      
      specificity_modelo.modelo <- mean(eval.modesta@TNR/(eval.modesta@FPR+eval.modesta@TNR)) #Especificidad
      
      TSS_modelo.model <- mean(eval.modesta@TPR+eval.modesta@TNR-1) #TSS
      
      return(c(auc_modelo.model,cor_modelo.model,kappa_modelo.model,sensibility_modelo.model,specificity_modelo.modelo,TSS_modelo.model))
}

# Hago una matriz para guardar los valores de los coeficientes de las diez iteraciones que me devuelva la función 
cosites_glm <- matrix(ncol=6,nrow=10) 

# Genereo un bucle for de 10 iteraciones que me devuelva 10 valores diferentes de los 6 coeficientes.
for (i in 1:10) {cosites_glm[i,] <- fddmx2(sdmdata[,c(6,7)],sdmdata,backgr,glm1)}

# Le doy nombre a las columnas para saber qué coeficientes hay dentro
colnames(cosites_glm) <- c('AUC','COR','Kappa','Sensitivity','Specificity','TSS')

# Hago la media por columnas que es lo que me interesa y lo guardo
coef_glm_model <- apply(cosites_glm, 2, mean);coef_glm_model 
write.csv(coef_glm_model,'C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/GLM Frecuentista/coeficientes_glmfrec.csv')
coef_glm <- read.csv('C:/Users/Irene/Source/Repositorios/TFM-Irene-Extremera/GLM frecuentista/coeficientes_glmfrec.csv')

```

```{r}

MoranI <- moran.test(residuals(glm1), listw=listw, randomisation=FALSE); MoranI #p valor de 1 indica que Ho no se rechaza, no hay autocorrelación espacial
Moran_MC <- moran.mc(residuals(glm1), listw=listw, nsim=100); Moran_MC #p valor de 0.99 Ho no se rechaza, lo cual indica que no hay autocorrelación espacial

```

No hay correlación espacial. Lo cual puede ser debido a que el area de estudio es muy grande.


###################################################
############# ----- Comparación ----- #############
###################################################

```{r}
matriz <- matrix(ncol=6,nrow=5,c(coef_bc_model,coef_mx_model,coef_brt_model,coef_rf_model,coef_glm_model))
colnames(matriz) <- c("AUC","COR","Kappa","Sensitivity","Specificity","TSS")
rownames(matriz) <- c("BIOCLIM","MAXENT","BRT","RF","GLM")
matriz
```




